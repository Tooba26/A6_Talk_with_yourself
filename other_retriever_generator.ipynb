{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load CV PDF\n",
    "pdf_loader = PyPDFLoader(\"CV_Tooba_Mehboob.pdf\")  # Change this to the correct file path\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "\n",
    "# Split text into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://tooba-portfolio.vercel.app/#portfolio\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text_data = soup.get_text()  # Extract raw text\n",
    "else:\n",
    "    text_data = \"Portfolio could not be fetched.\"\n",
    "\n",
    "# Save portfolio text as a document\n",
    "portfolio_docs = [text_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert `split_docs` and `portfolio_docs` into a list of strings (page_content)\n",
    "cv_docs = [Document(page_content=d.page_content) for d in split_docs]  # Extract `page_content` from each Document\n",
    "portfolio_docs = [Document(page_content=d) for d in portfolio_docs]  # Assuming `portfolio_docs` is already a list of strings\n",
    "\n",
    "# Merge the documents\n",
    "all_docs = cv_docs + portfolio_docs\n",
    "\n",
    "# Verify the content\n",
    "print(type(all_docs))  # Should be <class 'list'>\n",
    "print(type(all_docs[0]))  # Each entry should be a Document\n",
    "print(all_docs[0].page_content)  # Should print the text content of the first document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618aac435bb348a79308b51f605657db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6e7fce056a4fe4beb7791150a78902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de2f1d577bd406db0d221502326eba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ef573cd7994b219844637f953a91ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428847243498402099b9f5e2cbafb5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d780d51bacfc4064ba5aec089a050565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e6a20ca5ed424392d6383581054548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea337382de240029ba61fd076b8c9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6545c10f6344f79b50c90145b5c891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2ba4a328034bcb99dded2838cdd161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dbf95ed7fa4a7ebcbd82078fe16b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42ddfadf91a4ba4b4aa2ac983258479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f129ac1a5802450fbd62ac250eff3844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e771899ceef464caa2e1f29fc83d1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3e6495cb0a4663afc7443ce1d12ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ccb5e12a9e45dcb293fc42c7f740f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42440dd87c18400b8393d32a8f8b51ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c3653cb0ee43d9adb0a1f2fbf5a174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.xml:   0%|          | 0.00/211k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ded1c567fc43958f28ff612fed238b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ccd99fe8734e2cb2adfbf4212c2283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.xml:   0%|          | 0.00/368k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d1c9b7c44e47569f260b6117669750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e341b2dc79304e79b26c71082f724d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9492e4cf3c1a42cf9235c380875b8f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d3d0291f3642418da00558b845733d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b243991d219343b8ac0eb7b71aa38857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c31eae1d8947958948a594bed9f97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6b8f1abad24a1dad28186f8756b938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95be77a471294125a0464c0aeaec6a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Or OpenAIEmbeddings\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store in FAISS (vector database)\n",
    "vector_db = FAISS.from_documents(all_docs, embedding_model)\n",
    "\n",
    "# Save the vector store\n",
    "vector_db.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vector_db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load Falcon-7B\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create text generation pipeline\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Wrap HuggingFace pipeline properly\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Define RetrievalQA with correct LLM\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # ✅ Now correctly wrapped\n",
    "    retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 525, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "SSC Al-Amanah Youth Academy \n",
      "Marks obtained: 920 out of 1100\n",
      "Final gradeA1 \n",
      "Mother tongue(s):  URDU \n",
      "ABOUT ME \n",
      "WORK EXPERIENCE\n",
      "EDUCATION AND TRAINING\n",
      "LANGUAGE SKILLS\n",
      "\n",
      "2023 \n",
      "4th Position in Academics – University of Engineering & Technology, Peshawar \n",
      "2015 \n",
      "1st Position in Academics – Muhammad Amir Memorial Academy \n",
      "2014 \n",
      "1st in Hand Writing Competition – Muhammad Amir Memorial Academy \n",
      "2013 \n",
      "Participation in Inter-school English Debate Competition – Aisha Institute of Modern Sciences for\n",
      "Girls \n",
      "2012 \n",
      "1st Position in Academics – Muhammad Amir Memorial Academy \n",
      "2011 \n",
      "2nd Position in Academics – Muhammad Amir Memorial Academy \n",
      "2010\n",
      "\n",
      "2010 \n",
      "Good Behaviour – Muhammad Amir Memorial Academy \n",
      "2010 \n",
      "3rd Position in Academics – Muhammad Amir Memorial Academy \n",
      "Organizing Member at event Kaawish - Literary and Debating Society, UET Peshawar \n",
      "Organizer at one day Seminar on \"Cyber Security and Digital Forensics\" by NCCS, UET Peshawar \n",
      "Served as Core Member as a Google Developer Student Club for the 2022 - 2023 academic year \n",
      "HONOURS AND AWARDS \n",
      "MANAGEMENT AND LEADERSHIP SKILLS\n",
      "\n",
      "Other language(s):   \n",
      "UNDERSTANDING SPEAKING WRITING\n",
      "Listening Reading Spoken production Spoken interaction\n",
      "ENGLISH C2 C2 C2 C2 C2\n",
      "Levels: A1 and A2: Basic user; B1 and B2: Independent user; C1 and C2: Proﬁcient user \n",
      "Web Technologies Fundamentals - HTML, CSS  Node.Js, React.Js  Microsoft Oﬃce  Zustand  React Hooks, React\n",
      "Redux  JavaScript  Git  Machine learning  Model training  Dataset Building  JQuery  Email Template\n",
      "Markup  Tailwind  Material Tailwind  Python\n",
      "\n",
      "Question: What is my highest level of education?\n",
      "Helpful Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test basic retrieval\n",
    "query = \"What is my highest level of education?\"\n",
    "response = qa_chain.run(query)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other Retriever and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tooba Mehboob \n",
      "Date of birth: 26/02/2002\n",
      " Nationality: Pakistani\n",
      " Gender: Female \n",
      " Phone number: (+92) 3465547722 (Mobile) \n",
      " \n",
      "Email address: toobamehboob36@gmail.com \n",
      " Address: Islamia Colony, Palosi Piran, 25130, Pakistan (Home) \n",
      " \n",
      "I am a highly motivated computer science graduate with a solid foundation in web technologies which includes HTML,\n",
      "CSS, Scss, TailwindCSS, Material Tailwind, JQuery, JavaScript, ReactJS, NodeJS, NextJS, Express, MongoDB, Zustand,\n",
      "Redux and API Integrations. My academic journey at University of Engineering and Technology, Peshawar, equipped\n",
      "me with a robust skill set, reinforced by hands-on experiences and coursework. During my bachelor's I have done\n",
      "project on Maize Seeds Species Classiﬁcation using Machine Learning. During my internship at software company, I\n",
      "contributed to web development projects, gaining practical expertise in ReactJS, NodeJS, NextJS, Express and\n",
      "MongoDB. Noteworthy achievements include development of HR software, SMS Tool Console, Data\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text() + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "cv_text = extract_text_from_pdf(\"CV_Tooba_Mehboob.pdf\")\n",
    "print(cv_text[:1000])  # Print first 1000 characters to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tooba MehboobYou need to enable JavaScript to run this app.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "portfolio_text = extract_text_from_website(\"https://tooba-portfolio.vercel.app/\")\n",
    "print(portfolio_text[:1000])  # Print first 1000 characters to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert extracted text into LangChain Documents\n",
    "cv_doc = Document(page_content=cv_text)\n",
    "portfolio_doc = Document(page_content=portfolio_text)\n",
    "\n",
    "# Create FAISS database\n",
    "vector_db = FAISS.from_documents([cv_doc, portfolio_doc], embedding_model)\n",
    "\n",
    "# Save vector database\n",
    "vector_db.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046607eb2f6c4a95a0187aaaaebbb40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3055b90418984955809f5159ac1eac46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize my academic goals in one sentence.\"\n",
    "input_text = f\"Question: {query} Answer:\"\n",
    "response = model.generate(tokenizer.encode(input_text, return_tensors=\"pt\"), max_length=100)\n",
    "print(tokenizer.decode(response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context):\n",
    "    \"\"\"Generate answer using FLAN-T5\"\"\"\n",
    "    input_text = f\"Question: {query} Context: {context} Answer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(**inputs, max_length=100)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1928 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Bachelor of Science in Computer Science\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Load FAISS Retriever\n",
    "retriever = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True).as_retriever()\n",
    "\n",
    "def rag_chatbot(query):\n",
    "    \"\"\"Retrieve relevant context and generate response\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"Sorry, I couldn't find relevant information.\"\n",
    "\n",
    "    # Combine retrieved documents into one context\n",
    "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Generate answer using FLAN-T5\n",
    "    response = generate_answer(query, context)\n",
    "    return response\n",
    "\n",
    "# Test chatbot\n",
    "query = \"What is my highest level of education?\"\n",
    "response = rag_chatbot(query)\n",
    "print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Tooba Mehboob\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "response = rag_chatbot(query)\n",
    "print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: CGPA: 3.57 out of 4.00 Final grade A Thesis Maize seeds species classification using Machine Learning\n"
     ]
    }
   ],
   "source": [
    "query = \"What are my academic achievements?\"\n",
    "response = rag_chatbot(query)\n",
    "print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: yes\n"
     ]
    }
   ],
   "source": [
    "query = \"Did i studied at Al amanah youth academy?\"\n",
    "response = rag_chatbot(query)\n",
    "print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I am a highly motivated computer science graduate with a solid foundation in web technologies\n"
     ]
    }
   ],
   "source": [
    "query = \"How good I am in studies?\"\n",
    "response = rag_chatbot(query)\n",
    "print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
